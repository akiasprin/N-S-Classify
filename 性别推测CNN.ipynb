{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "max_name_length = 6\n",
    "\n",
    "vocabulary_list = []\n",
    "vocabulary = {}\n",
    "\n",
    "vocab = None\n",
    "model = None\n",
    "train_x, train_x_vec, train_y = None, None, None\n",
    "vaild_x, vaild_x_vec, vaild_y = None, None, None\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    x, y = [], []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        first_line = True\n",
    "        for line in f:\n",
    "            if first_line is True:\n",
    "                first_line = False\n",
    "                continue\n",
    "            sample = line.strip().split(',')\n",
    "            if len(sample) == 2 and sample[0] is not None and len(sample[0]) > 0:\n",
    "                x.append(sample[0])\n",
    "                if sample[1] == '0':\n",
    "                    y.append([1, 0])  # 男\n",
    "                else:\n",
    "                    y.append([0, 1])  # 女\n",
    "    return x, y\n",
    "\n",
    "def read_data():\n",
    "    global train_x, train_x_vec, train_y\n",
    "    global vaild_x, vaild_x_vec, vaild_Y\n",
    "    train_x, train_y = read_file('data.csv')\n",
    "#     vaild_x, vaild_y = read_file('vaild.csv')\n",
    "#     train_x = train_x + vaild_x\n",
    "#     train_y = train_y + vaild_y\n",
    "#     data = pd.read_csv('name.csv', encoding='utf-8').values\n",
    "#     train_x, train_y = data[:,0:1], data[:,1:]\n",
    "#     train_x = np.array(train_x, dtype=np.object)\n",
    "#     train_x = train_x.flatten()\n",
    "#     train_y = np.array(train_y, dtype=np.int32)\n",
    "#     train_y = to_categorical(train_y, 2)\n",
    "#     data = pd.read_csv('csv.csv', encoding='utf-8').values\n",
    "#     vaild_x, vaild_y = data[:,0:1], data[:,1:]\n",
    "#     vaild_x = np.array(vaild_x, dtype=np.object)\n",
    "#     vaild_x = vaild_x.flatten()\n",
    "#     vaild_y = np.array(vaild_y, dtype=np.int32)\n",
    "#     vaild_y = to_categorical(vaild_y, 2)\n",
    "\n",
    "\n",
    "def build_cab():\n",
    "    global vocab, vocabulary_list, vocabulary\n",
    "    word_map = \"word.map\"\n",
    "    if os.path.exists(word_map):\n",
    "        with open(word_map, mode='rb') as fp:  \n",
    "            vocab = pickle.loads(fp.read()) \n",
    "    else:\n",
    "        counter = 0\n",
    "        for name in train_x:\n",
    "            if hasattr(name, '__iter__'):\n",
    "                counter += 1\n",
    "                tokens = [word for word in name]\n",
    "                for word in tokens:\n",
    "                    if word in vocabulary:\n",
    "                        vocabulary[word] += 1\n",
    "                    else:\n",
    "                        vocabulary[word] = 1\n",
    "            else:\n",
    "                print(\"wtf data??\")\n",
    "#         for name in vaild_x:\n",
    "#             if hasattr(name, '__iter__'):\n",
    "#                 counter += 1\n",
    "#                 tokens = [word for word in name]\n",
    "#                 for word in tokens:\n",
    "#                     if word in vocabulary:\n",
    "#                         vocabulary[word] += 1\n",
    "#                     else:\n",
    "#                         vocabulary[word] = 1\n",
    "#             else:\n",
    "#                 print(\"wtf data??\")\n",
    "        vocabulary_list = [' '] + sorted(vocabulary, key=vocabulary.get, reverse=True)\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(vocabulary_list)])\n",
    "        data = pickle.dumps(obj=vocab)  \n",
    "        with open(word_map, mode='wb') as fp:  \n",
    "            fp.write(data)\n",
    "\n",
    "def build_vec():\n",
    "    global train_x_vec\n",
    "    global vaild_x_vec\n",
    "    train_x_vec = []\n",
    "    for name in train_x:\n",
    "        name_vec = []\n",
    "        if hasattr(name, '__iter__'):\n",
    "            for i in range(min(max_name_length, len(name))):\n",
    "                name_vec.append(vocab.get(name[i]))\n",
    "        while len(name_vec) < max_name_length:\n",
    "            name_vec.append(0)\n",
    "        train_x_vec.append(name_vec)\n",
    "    train_x_vec = np.array(train_x_vec)\n",
    "#     vaild_x_vec = []\n",
    "#     for name in vaild_x:\n",
    "#         name_vec = []\n",
    "#         if hasattr(name, '__iter__'):\n",
    "#             for word in name:\n",
    "#                 name_vec.append(vocab.get(word))\n",
    "#         while len(name_vec) < max_name_length:\n",
    "#             name_vec.append(0)\n",
    "#         vaild_x_vec.append(name_vec)\n",
    "#     vaild_x_vec = np.array(vaild_x_vec)\n",
    "\n",
    "\n",
    "read_data()\n",
    "build_cab()\n",
    "build_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = None\n",
    "input_size = max_name_length\n",
    "num_classes = 2\n",
    "\n",
    "batch_size = 0\n",
    "num_batch = 0 \n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, input_size])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 算法:Convolutional Neural Network For Sentence Classification<Yoon Kim>\n",
    "def neural_network(vocabulary_size, embedding_size=128, num_filters=192):\n",
    "    # embedding layer:lookup-table\n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        # W:给每个字一个128大小的均匀分布:{h:n,w:128} => 对于每个字,字典有128个解释,初始分布平均([1,-1]记录影响强弱)\n",
    "        W = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        # embedded_chars:获取字Xi对应的Wi:{h:5,w:128}\n",
    "        embedded_chars = tf.nn.embedding_lookup(W, X) \n",
    "        # embedded_chars_expanded:展开分布获取通道:{h:5,w:128,c:1}\n",
    "        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    # filter_sizes:卷积核设计\n",
    "    # Q:为什么不设2? A:2只是多了一类AB的关联,而姓名和第一个字关联不大\n",
    "    filter_sizes = [i for i in range(3, input_size+1)]\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "            # filter_shape:卷积核形状:{h:[3,4,5],w:128,in_c:1,out_c:256}\n",
    "            filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "            # W:构造填充正太分布的卷积核:{h:[3,4,5],w:128,in_c:1,out_c:256} => 字典的解释影响概率\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "            # b:biases:添加使其变成非线性激活的常量:{l:256}\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "            # conv:卷积:{h:[3,2,1],w:128,c:192} => 将每相邻长度[3,4,5]字的解释糅杂一起\n",
    "            conv = tf.nn.conv2d(embedded_chars_expanded, W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            # h:relu激活:{h:[3,2,1],w:128}\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "            # pooled:池化:窗口:{h:[3,2,1],w:1}:{h:[1,1,1],w:128} => 求出影响概率最大的解释\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, input_size - filter_size + 1, 1, 1],\n",
    "                                    strides=[1, 1, 1, 1], padding='VALID')\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "    # num_filters_total:256*3=576\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    # h_pool_flat:所有解释影响概率扁平化:{l:576}\n",
    "    h_pool = tf.concat(pooled_outputs, 3)\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "    with tf.name_scope(\"output\"):\n",
    "        W = tf.get_variable(\"W\", shape=[num_filters_total, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        output = tf.nn.xw_plus_b(h_drop, W, b)\n",
    "        \n",
    "    return output\n",
    "\n",
    "output = neural_network(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211482, 1211482, 6974, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x_vec), len(train_y), len(vocab), len(vocabulary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 13461\n",
    "num_batch = len(train_x_vec) // batch_size\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "# 计算loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=Y))\n",
    "# 计算梯度\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "# 把梯度应用到变量\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(201):\n",
    "        for i in range(num_batch):\n",
    "            batch_x = train_x_vec[i*batch_size : (i+1)*batch_size]\n",
    "            batch_y = train_y[i*batch_size : (i+1)*batch_size]\n",
    "            loss_ = sess.run([train_op, loss], feed_dict={X:batch_x, Y:batch_y, dropout_keep_prob:0.5})\n",
    "            #print(e, i, loss_)\n",
    "        print(e, loss_)\n",
    "        \n",
    "        if e % 100 == 0:\n",
    "                saver.save(sess, \"./model/gender.ckpt\", global_step=e)\n",
    "    save_path = saver.save(sess, \"./model/gender.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 230, 810, 0, 0, 0], [5, 84, 183, 0, 0, 0], [154, 58, 176, 0, 0, 0], [10, 1411, 1872, 0, 0, 0], [1640, 2836, 3170, 2733, 2830, 0], [499, 6303, 299, 3518, 0, 0], [499, 6303, 299, 4438, 0, 0], [112, 7, 444, 0, 0, 0], [21, 270, 299, 0, 0, 0], [995, 356, 33, 0, 0, 0], [3, 290, 53, 0, 0, 0], [39, 1, 748, 137, 0, 0], [576, 1559, 0, 0, 0, 0], [770, 425, 0, 0, 0, 0], [5, 418, 1139, 0, 0, 0], [88, 366, 1348, 0, 0, 0], [69, 1707, 1142, 0, 0, 0], [2, 624, 257, 0, 0, 0], [19, 46, 221, 0, 0, 0], [3, 27, 104, 0, 0, 0], [87, 3767, 7, 0, 0, 0], [5, 137, 438, 742, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "name_list = [\"刘倩妤\", \"陈浩民\", \"邓子琪\", \"林鹭攸\", \"别说话吻我\", \"麻辣香菇\", \"麻辣香锅\", \n",
    "             \"蔡文姬\", \"孙尚香\", \"狄仁杰\", \"李元芳\", \"东王太一\", \"荆轲\", \"阿柯\",\n",
    "             \"陈奕迅\", \"方皓玟\", \"宋黛霆\", \"张信哲\", \"徐佳莹\", \"李玉刚\", \"叶蒨文\", \"陈一发儿\"]\n",
    "for name in name_list:\n",
    "    name_vec = []\n",
    "    if hasattr(name, '__iter__'):\n",
    "        for word in name:\n",
    "            if vocab.get(word) != None:\n",
    "                name_vec.append(vocab.get(word))\n",
    "        while len(name_vec) < max_name_length:\n",
    "            name_vec.append(0)\n",
    "        x.append(name_vec)\n",
    "print(x)\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/gender.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/gender.ckpt\n",
      "刘倩妤 女\n",
      "陈浩民 男\n",
      "邓子琪 女\n",
      "林鹭攸 女\n",
      "别说话吻我 女\n",
      "麻辣香菇 女\n",
      "麻辣香锅 女\n",
      "蔡文姬 女\n",
      "孙尚香 女\n",
      "狄仁杰 男\n",
      "李元芳 女\n",
      "东王太一 男\n",
      "荆轲 男\n",
      "阿柯 男\n",
      "陈奕迅 男\n",
      "方皓玟 女\n",
      "宋黛霆 男\n",
      "张信哲 男\n",
      "徐佳莹 女\n",
      "李玉刚 男\n",
      "叶蒨文 女\n",
      "陈一发儿 男\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "with tf.Session() as sess:\n",
    "    # 恢复前一次训练\n",
    "    ckpt = tf.train.get_checkpoint_state('./model/')\n",
    "    if ckpt != None:\n",
    "        print(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"没找到模型\")\n",
    "        \n",
    "    res = sess.run(output, {X:x, dropout_keep_prob:1.0})\n",
    "\n",
    "#     i = 0\n",
    "#     for name in name_list:\n",
    "#         print(name, res[i])\n",
    "#         i += 1\n",
    "    predictions = tf.argmax(output, 1)\n",
    "    res = sess.run(predictions, {X:x, dropout_keep_prob:0.5})\n",
    "\n",
    "    i = 0\n",
    "    for name in name_list:\n",
    "        print(name, '女' if res[i] == 1 else '男')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续训练\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state('./model/')\n",
    "    if ckpt != None:\n",
    "        print(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        for e in range(501, 801):\n",
    "            for i in range(num_batch):\n",
    "                batch_x = train_x_vec[i*batch_size : (i+1)*batch_size]\n",
    "                batch_y = train_y[i*batch_size : (i+1)*batch_size]\n",
    "                loss_ = sess.run([train_op, loss], feed_dict={X:batch_x, Y:batch_y, dropout_keep_prob:0.5})\n",
    "                #print(e, i, loss_)\n",
    "            print(e, loss_)\n",
    "            if e % 100 == 0:\n",
    "                    saver.save(sess, \"./model/gender.ckpt\", global_step=e)\n",
    "        save_path = saver.save(sess, \"./model/gender.ckpt\")\n",
    "        print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/gender.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/gender.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 训练集准确率计算\n",
    "acc = 0\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state('./model/')\n",
    "    if ckpt != None:\n",
    "        print(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"没找到模型\")\n",
    "        \n",
    "    predictions = tf.argmax(output, 1)\n",
    "    for i in range(num_batch):\n",
    "        batch_x = train_x_vec[i*batch_size : (i+1)*batch_size]\n",
    "        batch_y = train_y[i*batch_size : (i+1)*batch_size]\n",
    "        res = sess.run(predictions, feed_dict={X:batch_x, dropout_keep_prob:1.0})\n",
    "\n",
    "        for j in range(len(res)):\n",
    "            k = i*batch_size+j\n",
    "            if train_y[k][res[j]]:\n",
    "                acc = acc + 1\n",
    "            else:\n",
    "                pass\n",
    "#                print(train_x[k], '女' if res[j] == 1 else '男', '女' if train_y[k][1] else '男')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1084493, 1211482, 0.8951787975388821)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, len(train_y), acc/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_x)):\n",
    "    for j in train_x[i]:\n",
    "        if j == ' ':\n",
    "            print(i)\n",
    "            \n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
